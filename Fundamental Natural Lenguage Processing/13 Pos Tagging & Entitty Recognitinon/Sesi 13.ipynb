{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Sesi 13"],"metadata":{"id":"w0Rq46DrV8_g"}},{"cell_type":"markdown","source":["# POS Tagging"],"metadata":{"id":"NdUXAjNvWBGx"}},{"cell_type":"markdown","source":["Pada pembuatan POS Tagging kali ini, Algoritma yang akan digunakan adalah Random Forest"],"metadata":{"id":"1Et4z0nymA1v"}},{"cell_type":"code","source":["!pip install nlp_id  # untuk kepeluan tokenizer, bukan postag"],"metadata":{"id":"4ZRHDY3yWGSm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","import os\n","import pickle\n","import warnings\n","import wget\n","from nlp_id.tokenizer import Tokenizer\n","from nltk.tree import Tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.pipeline import Pipeline\n","\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PCUKoHqiX2mc","outputId":"cdcf5605-d066-48ba-e87a-a310cdf56bc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["tokenizer = Tokenizer()"],"metadata":{"id":"gUosdABJYFfJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_path = \"/content/dataset_postag.txt\""],"metadata":{"id":"cKVkX5hOZ3ou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_dataset(dataset_path=None):\n","\n","    with open(dataset_path) as f:\n","        raw_file = f.read().split(\"\\n\")\n","\n","    files = [i.split(\"\\t\") for i in raw_file]\n","\n","    sentences, tags, temp_sentences, temp_tags = [], [], [], []\n","\n","    for file in files:\n","        if file != [\"\"]:\n","            temp_sentences.append(file[0])  # get the sentences\n","            temp_tags.append(file[1])  # get the tag\n","        else:\n","            # check if the temp sentences and temp tags is not null\n","            # and both of them have the same length\n","            if len(temp_sentences) > 0 and (\n","                len(temp_sentences) == len(temp_tags)\n","            ):\n","                sentences.append(temp_sentences)\n","                tags.append(temp_tags)\n","            temp_sentences, temp_tags = [], []\n","    return sentences, tags"],"metadata":{"id":"1JCBeHXVYfZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_dataset(dataset_path)"],"metadata":{"id":"_l85WWMGZ7Jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences, tags = read_dataset(dataset_path)"],"metadata":{"id":"03QNnU5dZ-Yn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def features(sentence, index):\n","    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n","    return {\n","        \"word\": sentence[index],\n","        \"is_first\": index == 0,\n","        \"is_last\": index == len(sentence) - 1,\n","        \"is_capitalized\": sentence[index][0].upper()\n","        == sentence[index][0],\n","        \"is_all_caps\": sentence[index].upper() == sentence[index],\n","        \"is_all_lower\": sentence[index].lower() == sentence[index],\n","        \"has_hyphen\": \"-\" in sentence[index],\n","        \"is_numeric\": sentence[index].isdigit(),\n","        \"capitals_inside\": sentence[index][1:].lower()\n","        != sentence[index][1:],\n","        \"prefix-1\": sentence[index][0],\n","        \"prefix-1-lower\": sentence[index][0].lower(),\n","        \"prefix-2\": sentence[index][:2],\n","        \"prefix-2-lower\": sentence[index][:2].lower(),\n","        \"prefix-3\": sentence[index][:3],\n","        \"prefix-3-lower\": sentence[index][:3].lower(),\n","        \"suffix-1\": sentence[index][-1],\n","        \"suffix-1-lower\": sentence[index][-1].lower(),\n","        \"suffix-2\": sentence[index][-2:],\n","        \"suffix-2-lower\": sentence[index][-2:].lower(),\n","        \"suffix-3\": sentence[index][-3:],\n","        \"suffix-3-lower\": sentence[index][-3:].lower(),\n","        \"lowercase_word\": sentence[index].lower(),\n","        \"prev_word\": \"\" if index == 0 else sentence[index - 1],\n","        \"next_word\": \"\"\n","        if index == len(sentence) - 1\n","        else sentence[index + 1],\n","        \"prev_word_is_capitalized\": False\n","        if index == 0\n","        else sentence[index - 1][0].upper() == sentence[index - 1][0],\n","        \"next_word_is_capitalized\": False\n","        if index == len(sentence) - 1\n","        else sentence[index + 1][0].upper() == sentence[index + 1][0],\n","        \"2-prev-word\": \"\" if index <= 1 else sentence[index - 2],\n","        \"2-next-word\": \"\"\n","        if index >= len(sentence) - 2\n","        else sentence[index + 2],\n","    }"],"metadata":{"id":"yuV8F-4LbeNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def transform_to_dataset(sentences, tags):\n","    X, y = [], []\n","\n","    for sentence_idx in range(len(sentences)):\n","        for index in range(len(sentences[sentence_idx])):\n","            X.append(features(sentences[sentence_idx], index))\n","            y.append(tags[sentence_idx][index])\n","\n","    return X, y"],"metadata":{"id":"NatKzPWdbLgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf = Pipeline(\n","        [\n","            (\"vectorizer\", DictVectorizer(sparse=True)),\n","            (\n","                \"classifier\",\n","                RandomForestClassifier(\n","                    criterion=\"gini\", n_estimators=15, random_state=2020\n","                ),\n","            ),\n","        ]\n","    )"],"metadata":{"id":"vGIMjTNgfk9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(sentences, tags):\n","    \"\"\"\n","    training\n","    \"\"\"\n","    clf.fit(sentences, tags)"],"metadata":{"id":"uLpct4dnauh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences, tags = transform_to_dataset(sentences, tags)"],"metadata":{"id":"GSMhzIBWa5ww"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(sentences, tags)"],"metadata":{"id":"H7pgV3RBa8MM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_model(model_path):\n","    pickle_out = open(model_path, \"wb\")\n","    pickle.dump(clf, pickle_out)\n","    pickle_out.close()"],"metadata":{"id":"1PqvDu45b6EP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = \"/content/postagger_model.pkl\"\n","save_model(model_path)"],"metadata":{"id":"an9NwKmLi0md"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_model(model_path):\n","    pickle_in = open(model_path, \"rb\")\n","    load_data = pickle.load(pickle_in)\n","    return load_data"],"metadata":{"id":"LckJeqlfi9y7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = load_model(model_path)"],"metadata":{"id":"yzZU8HwTjWo6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_pos_tag(text):\n","    result = []\n","    sents = nltk.sent_tokenize(text)\n","    symbols = ['!', '&', '(', ')', '*', '?', ',', '.', '<', '>', '/', ':', ';',\n","                '[', ']', '\\\\', '^', '`', '{', '}', '|', '~', '\"', 'â€œ', \"'\"]\n","    for sent in sents:\n","        tokenized_word = tokenizer.tokenize(sent)\n","        if sent:\n","            tags = model.predict(\n","                [\n","                    features(tokenized_word, index)\n","                    for index in range(len(tokenized_word))\n","                ]\n","            )\n","            for i in range(len(tags)):\n","                if tokenized_word[i] in symbols:\n","                    result.append((tokenized_word[i], \"SYM\"))\n","                else:\n","                    result.append((tokenized_word[i], tags[i]))\n","    return result"],"metadata":{"id":"s_YfThfnjePE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"Lionel Messi pergi ke pasar di daerah Jakarta Pusat.\""],"metadata":{"id":"Yu-bc_qUjzSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_pos_tag(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UMIgHr4hkG-t","outputId":"be2bfe7d-e1b7-428b-bac0-df635ff8a52b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Lionel', 'NNP'),\n"," ('Messi', 'NNP'),\n"," ('pergi', 'VB'),\n"," ('ke', 'IN'),\n"," ('pasar', 'NN'),\n"," ('di', 'IN'),\n"," ('daerah', 'NN'),\n"," ('Jakarta', 'NNP'),\n"," ('Pusat', 'NNP'),\n"," ('.', 'SYM')]"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":[],"metadata":{"id":"hVQ4WOHkkJ3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# NER"],"metadata":{"id":"KPO8Sm4Ml905"}},{"cell_type":"markdown","source":["Pada pembuatan NER kali ini, Tool yang digunakan adalah Spacy"],"metadata":{"id":"scA7SyXzmQoM"}},{"cell_type":"code","source":["import pickle\n","import spacy\n","import random\n","from spacy.util import minibatch, compounding\n","from spacy import load, displacy\n","from spacy.training.example import Example"],"metadata":{"id":"piIkDkDImOMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/dataset_ner_spacy.pickle', 'rb') as f:\n","    data = pickle.load(f)"],"metadata":{"id":"QaZflZ-yl--Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.blank(\"id\")\n","nlp.add_pipe('ner')\n","nlp.begin_training()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23FOqk3Bmkkd","outputId":"6d06c43a-494a-40de-ec14-8f297fe0c9ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<thinc.optimizers.Optimizer at 0x7fe597766340>"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["ner = nlp.get_pipe(\"ner\")"],"metadata":{"id":"t7WPudLknGLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _, annotations in data:\n","    for ent in annotations.get(\"entities\"):\n","        ner.add_label(ent[2])\n","        break"],"metadata":{"id":"jPSeCsZknKIy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n","unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"],"metadata":{"id":"NDitKdBInRs0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training model\n","with nlp.disable_pipes(*unaffected_pipes):\n","\n","  # training for 30 iterations\n","  for iteration in range(30):\n","\n","    # shuufling examples  before every iteration\n","    random.shuffle(data)\n","    losses = {}\n","    # batch up the examples using spaCy's minibatch\n","    batches = minibatch(data, size=compounding(4.0, 32.0, 1.001))\n","    for batch in batches:\n","        texts, annotations = zip(*batch)\n","        example = []\n","        # Update the model with iterating each text\n","        for i in range(len(texts)):\n","            doc = nlp.make_doc(texts[i])\n","            example.append(Example.from_dict(doc, annotations[i]))\n","        nlp.update(\n","            example,\n","            drop=0.5,  # dropout - make it harder to memorise data\n","            losses=losses,\n","            )\n","\n","    print(\"Losses at iteration {}\".format(iteration), losses)"],"metadata":{"id":"GjpuEYzxnUhB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test\n","doc = nlp(\"Lionel Messi pergi ke pasar di daerah Jakarta Pusat.\")\n","\n","print(doc.ents)\n","print(\"----\")\n","print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QFrzSMsneSR","outputId":"b67d9044-c2c1-4f45-b354-b81775104607"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(Lionel Messi, Jakarta Pusat)\n","----\n","Entities [('Lionel Messi', 'PERSON'), ('Jakarta Pusat', 'LOCATION')]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"znDRHdZzsi-z"},"execution_count":null,"outputs":[]}]}