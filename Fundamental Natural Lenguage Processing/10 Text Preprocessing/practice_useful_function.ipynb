{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text = \"<html><body><h1>Hello< World!\\nHello you!</h1></body></html>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def extract_from_html_using_parser(html_text):\n",
    "\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    extracted_text = soup.get_text()\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello< World!\\nHello you!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_parser_result = extract_from_html_using_parser(html_text)\n",
    "display(html_parser_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_word_only(text):\n",
    "\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = cleaned_text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['htmlbodyh1Hello', 'World', 'Hello', 'youh1bodyhtml']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "only_extraxt_text = get_word_only(html_text)\n",
    "display(only_extraxt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(row_html):\n",
    "    CLEANER = re.compile('<.*?>')\n",
    "\n",
    "    cleantext = re.sub(CLEANER, '', row_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello< World!\\nHello you!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_html_tags = clean_html(html_text)\n",
    "display(cleaned_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"INi adalah contoh tweet dengan #hashtag dan ðŸ—¿ emoji\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \n",
    "    tweet = re.sub(r'#\\w+', '', tweet) # remove hasgtag\n",
    "    tweet = re.sub(r'[^\\w\\s]+', '', tweet) # remove no-alphanumeric\n",
    "    tweet = re.sub(r'\\s+', \" \", tweet) # remove extra spaces\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INi adalah contoh tweet dengan #hashtag dan ðŸ—¿ emoji'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'INi adalah contoh tweet dengan dan emoji'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_tweet = clean_tweet(tweet)\n",
    "display(tweet)\n",
    "display(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def clean_tweet_with_unidecode(tweet):\n",
    "\n",
    "    tweet = re.sub(r\"#\\w+\", \"\", tweet)\n",
    "    tweet = unidecode(tweet)\n",
    "    tweet = re.sub(r\"\\s+\", \" \", tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INi adalah contoh tweet dengan #hashtag dan ðŸ—¿ emoji'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'INi adalah contoh tweet dengan dan emoji'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_tweet = clean_tweet_with_unidecode(tweet)\n",
    "\n",
    "display(tweet)\n",
    "display(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = \"I am reading a book in the park.\"\n",
    "indonesian_text = \"Saya sedang membaca buku di taman.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def remove_stopword_english(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am reading a book in the park.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'reading book park .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eng_stopwords_removal = remove_stopword_english(english_text)\n",
    "\n",
    "display(english_text)\n",
    "display(eng_stopwords_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword_indonesian(text):\n",
    "\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_text = [token for token in tokens if token.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(filtered_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saya sedang membaca buku di taman.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'membaca buku taman .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_stopwords_removal = remove_stopword_indonesian(indonesian_text)\n",
    "\n",
    "display(indonesian_text)\n",
    "display(id_stopwords_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saya sedang membaca buku di taman.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Saya sedang membaca buku di taman'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "punctuation_removal = remove_punctuation(indonesian_text)\n",
    "\n",
    "display(indonesian_text)\n",
    "display(punctuation_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Special Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP seru banget! tapi susah juga ya~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_character(text):\n",
    "\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP seru banget! tapi susah juga ya~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'NLP seru banget tapi susah juga ya'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "special_character_removal = remove_special_character(text)\n",
    "\n",
    "display(text)\n",
    "display(special_character_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove emoji dan emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"ðŸ˜ƒIni adalah contoh tweet dengan ðŸ˜ƒ ðŸ˜ƒ ðŸ˜ƒ emoji!ðŸ˜ƒðŸ‘Œ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji_emoticon(text):\n",
    "    # Kode untuk menghapus emoji dan emoticon\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    cleaned_text = emoji_pattern.sub(r'', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ˜ƒIni adalah contoh tweet dengan ðŸ˜ƒ ðŸ˜ƒ ðŸ˜ƒ emoji!ðŸ˜ƒðŸ‘Œ'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Ini adalah contoh tweet dengan    emoji'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emoji_emoticon_removal = remove_special_character(tweet)\n",
    "\n",
    "display(tweet)\n",
    "display(emoji_emoticon_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conver emoji dan emoticon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji_emoticon(text, emoji_dict):\n",
    "\n",
    "    for emoticon, replacment in emoji_dict.items():\n",
    "        text = text.replace(emoticon, replacment)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ˜ƒIni adalah contoh tweet dengan ðŸ˜ƒ ðŸ˜ƒ ðŸ˜ƒ emoji!ðŸ˜ƒðŸ‘Œ'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'smiling_face_with_open_mouthIni adalah contoh tweet dengan smiling_face_with_open_mouth smiling_face_with_open_mouth smiling_face_with_open_mouth emoji!smiling_face_with_open_mouthok_hand'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emoji_dict = {\n",
    "    \"ðŸ˜€\": \"grinning_face\",\n",
    "    \"ðŸ˜ƒ\": \"smiling_face_with_open_mouth\",\n",
    "    \"ðŸ¥±\": \"yawning_face\",\n",
    "    \"ðŸ‘Œ\": \"ok_hand\",\n",
    "    \"ðŸ‘\": \"thumbs_up\",\n",
    "    \"ðŸ‘Ž\": \"thumbs_down\",\n",
    "    \"ðŸ™\": \"folded_hands\"\n",
    "}\n",
    "\n",
    "emoji_emoticon_convertion = convert_emoji_emoticon(tweet, emoji_dict)\n",
    "\n",
    "display(tweet)\n",
    "display(emoji_emoticon_convertion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Folding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ini adalah sebuah text untuk contoh CASE FOLDING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ini adalah sebuah text untuk contoh CASE FOLDING'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ini adalah sebuah text untuk contoh case folding'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'INI ADALAH SEBUAH TEXT UNTUK CONTOH CASE FOLDING'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Ini Adalah Sebuah Text Untuk Contoh Case Folding'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(text)\n",
    "display(text.lower())\n",
    "display(text.upper())\n",
    "display(text.title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text normaliazation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming_english(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am reading a book in the park.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'i am read a book in the park.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stemming_eng = stemming_english(english_text)\n",
    "\n",
    "display(english_text)\n",
    "display(stemming_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am running in the park and playing with runners.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hammam\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatozation_english(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am running in the park and playing with runners.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'I am running in the park and playing with runner .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmatized_eng = lemmatozation_english(text)\n",
    "\n",
    "display(text)\n",
    "display(lemmatized_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def word_tokenization(text):\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am running in the park and playing with runners.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'running',\n",
       " 'in',\n",
       " 'the',\n",
       " 'park',\n",
       " 'and',\n",
       " 'playing',\n",
       " 'with',\n",
       " 'runners',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_tokenized = word_tokenization(text)\n",
    "\n",
    "display(text)\n",
    "display(word_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Cats are cute and playful. They like to run and jump.\"\n",
    "\n",
    "def sentence_tokenization(text):\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cats are cute and playful. They like to run and jump.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Cats are cute and playful.', 'They like to run and jump.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent_toknized = sentence_tokenization(text)\n",
    "\n",
    "display(text)\n",
    "display(sent_toknized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text = \"The price of the item is $20.50. It's a great deal! Buy now for $15 only. Limited time offer!\"\n",
    "\n",
    "def regex_tokenization(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\$\\d+\\.?\\d*')\n",
    "    dollar_ammounts = tokenizer.tokenize(text)\n",
    "    return dollar_ammounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The price of the item is $20.50. It's a great deal! Buy now for $15 only. Limited time offer!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['$20.50', '$15']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regex_tokenized = regex_tokenization(text)\n",
    "\n",
    "display(text)\n",
    "display(regex_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"Ini adalah contoh tweet dentgan #hashtag dan ðŸ—¿ emoji\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_tokenization(tweet):\n",
    "    tokens = nltk.tokenize.TweetTokenizer().tokenize(tweet)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ini adalah contoh tweet dentgan #hashtag dan ðŸ—¿ emoji'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'tweet', 'dentgan', '#', 'hashtag', 'dan', 'ðŸ—¿', 'emoji']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ini',\n",
       " 'adalah',\n",
       " 'contoh',\n",
       " 'tweet',\n",
       " 'dentgan',\n",
       " '#hashtag',\n",
       " 'dan',\n",
       " 'ðŸ—¿',\n",
       " 'emoji']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweet_tokenized = tweet_tokenization(tweet)\n",
    "\n",
    "display(tweet)\n",
    "print(word_tokenize(tweet))\n",
    "display(tweet_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"cats are cute and playful. They like to run and jump.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_tokenization(text, n):\n",
    "\n",
    "    tokens = nltk.ngrams(text.split(), n)\n",
    "    return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cats are cute and playful. They like to run and jump.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'are', 'cute', 'and', 'playful', '.', 'They', 'like', 'to', 'run', 'and', 'jump', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cats', 'are', 'cute'),\n",
       " ('are', 'cute', 'and'),\n",
       " ('cute', 'and', 'playful.'),\n",
       " ('and', 'playful.', 'They'),\n",
       " ('playful.', 'They', 'like'),\n",
       " ('They', 'like', 'to'),\n",
       " ('like', 'to', 'run'),\n",
       " ('to', 'run', 'and'),\n",
       " ('run', 'and', 'jump.')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ngram_tokenized = ngram_tokenization(text, 3)\n",
    "\n",
    "display(text)\n",
    "print(word_tokenize(text))\n",
    "display(ngram_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
