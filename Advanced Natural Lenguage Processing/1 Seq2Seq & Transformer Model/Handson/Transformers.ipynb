{"cells":[{"cell_type":"markdown","metadata":{"id":"stj_XSs2f-Zs"},"source":["## 1. Check if GPU available & install package"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2677,"status":"ok","timestamp":1689985744831,"user":{"displayName":"Muhammad Fhadli","userId":"01580010850334259158"},"user_tz":-540},"id":"P45f_gaejgu7","outputId":"c187e48d-42b8-47c8-9288-2f94d40da2f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["there are 1 GPU(s) available.\n","we will use the GPU:  Tesla T4\n"]}],"source":["import torch\n","\n","if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","\n","  print('there are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","  print('we will use the GPU: ', torch.cuda.get_device_name(0))\n","\n","else:\n","  print(\"No GPU available, using the CPU instead\")\n","  device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10386,"status":"ok","timestamp":1689985755215,"user":{"displayName":"Muhammad Fhadli","userId":"01580010850334259158"},"user_tz":-540},"id":"B5orWLXIXYaj","outputId":"f754b5f2-9d5c-47f5-b4ed-36ba18b78948"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: OpenNMT-tf[tensorflow] in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: ctranslate2<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (3.17.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (23.1)\n","Requirement already satisfied: pyonmttok<2,>=1.29.0 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (1.37.1)\n","Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (6.0.1)\n","Requirement already satisfied: rouge<2,>=1.0 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (1.0.1)\n","Requirement already satisfied: sacrebleu<3,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (2.3.1)\n","Requirement already satisfied: tensorflow-addons<0.20,>=0.16 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (0.19.0)\n","Requirement already satisfied: tensorflow<2.12.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (2.11.1)\n","Requirement already satisfied: tensorflow-text<2.12.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-tf[tensorflow]) (2.11.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.0->OpenNMT-tf[tensorflow]) (1.22.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge<2,>=1.0->OpenNMT-tf[tensorflow]) (1.16.0)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3,>=1.5.0->OpenNMT-tf[tensorflow]) (2.7.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3,>=1.5.0->OpenNMT-tf[tensorflow]) (2022.10.31)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3,>=1.5.0->OpenNMT-tf[tensorflow]) (0.9.0)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3,>=1.5.0->OpenNMT-tf[tensorflow]) (0.4.6)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<3,>=1.5.0->OpenNMT-tf[tensorflow]) (4.9.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.56.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.8.0)\n","Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (16.0.6)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.3.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.19.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (67.7.2)\n","Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.11.2)\n","Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.11.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (4.7.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.32.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons<0.20,>=0.16->OpenNMT-tf[tensorflow]) (4.0.0)\n","Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.40.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.3.6)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12.0,>=2.6.0->OpenNMT-tf[tensorflow]) (3.2.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install --upgrade pip\n","!pip install OpenNMT-tf[tensorflow]"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPS5l6irXbdh","outputId":"9d644d58-0de2-4045-a70e-cbfd0fb98504","executionInfo":{"status":"ok","timestamp":1689985766120,"user_tz":-540,"elapsed":10912,"user":{"displayName":"Muhammad Fhadli","userId":"01580010850334259158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-22 00:29:15.003336: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-22 00:29:16.084489: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-07-22 00:29:16.084645: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-07-22 00:29:16.084680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","OpenNMT-tf\n","2.31.0\n"]}],"source":["!onmt-main -v"]},{"cell_type":"markdown","metadata":{"id":"MTZ1WE3JjrN-"},"source":["## 2. Download & Extract data"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Bqx2lLd2ZSW3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689985770146,"user_tz":-540,"elapsed":4050,"user":{"displayName":"Muhammad Fhadli","userId":"01580010850334259158"}},"outputId":"2ae29fa4-0192-4f77-acb1-f05c1ca8408f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install wget"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6-oeXhNCXy4U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689985770945,"user_tz":-540,"elapsed":806,"user":{"displayName":"Muhammad Fhadli","userId":"01580010850334259158"}},"outputId":"c4284f00-7b1c-4f74-f68e-2712acd1bacc"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-07-22 00:29:29--  https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.34.224, 54.231.225.64, 52.216.50.184, ...\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.34.224|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1662081 (1.6M) [application/x-gzip]\n","Saving to: ‘toy-ende.tar.gz.2’\n","\n","\rtoy-ende.tar.gz.2     0%[                    ]       0  --.-KB/s               \rtoy-ende.tar.gz.2   100%[===================>]   1.58M  --.-KB/s    in 0.1s    \n","\n","2023-07-22 00:29:30 (14.2 MB/s) - ‘toy-ende.tar.gz.2’ saved [1662081/1662081]\n","\n"]}],"source":["!wget https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz\n","!tar xf toy-ende.tar.gz"]},{"cell_type":"markdown","metadata":{"id":"cRgrbR8fj35i"},"source":["## 3. Create vocabulary"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"vbDnv8BJZdk1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689985787633,"user_tz":-540,"elapsed":16690,"user":{"displayName":"Muhammad Fhadli","userId":"01580010850334259158"}},"outputId":"e1fe82c5-3f6a-4c1c-b751-898dab6ca0a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-22 00:29:30.326410: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-22 00:29:31.367071: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-07-22 00:29:31.367169: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-07-22 00:29:31.367188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","2023-07-22 00:29:37.932000: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2023-07-22 00:29:40.057691: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-22 00:29:41.584322: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-07-22 00:29:41.584447: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-07-22 00:29:41.584471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","2023-07-22 00:29:45.768555: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n"]}],"source":["!onmt-build-vocab --size 1000 --save_vocab src-vocab.txt toy-ende/src-train.txt\n","!onmt-build-vocab --size 1000 --save_vocab tgt-vocab.txt toy-ende/tgt-train.txt"]},{"cell_type":"markdown","metadata":{"id":"hil1qnTgj-xi"},"source":["## 4. Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHVvJ3dvZqXm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"61c2f739-5d0a-4534-e0d2-99be5bd0ae7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-22 00:29:47.222871: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-22 00:29:48.290045: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-07-22 00:29:48.290143: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-07-22 00:29:48.290163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","2023-07-22 00:29:52.690000: I main.py:315] Using OpenNMT-tf version 2.31.0\n","2023-07-22 00:29:52.690000: I main.py:315] Using model:\n","(model): TransformerBase(\n","  (examples_inputter): SequenceToSequenceInputter(\n","    (features_inputter): WordEmbedder()\n","    (labels_inputter): WordEmbedder()\n","    (inputters): ListWrapper(\n","      (0): WordEmbedder()\n","      (1): WordEmbedder()\n","    )\n","  )\n","  (encoder): SelfAttentionEncoder(\n","    (position_encoder): SinusoidalPositionEncoder(\n","      (reducer): SumReducer()\n","    )\n","    (layer_norm): LayerNorm()\n","    (layers): ListWrapper(\n","      (0): SelfAttentionEncoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","      (1): SelfAttentionEncoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","      (2): SelfAttentionEncoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","      (3): SelfAttentionEncoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","      (4): SelfAttentionEncoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","      (5): SelfAttentionEncoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","    )\n","  )\n","  (decoder): SelfAttentionDecoder(\n","    (position_encoder): SinusoidalPositionEncoder(\n","      (reducer): SumReducer()\n","    )\n","    (layer_norm): LayerNorm()\n","    (layers): ListWrapper(\n","      (0): SelfAttentionDecoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (attention): ListWrapper(\n","          (0): TransformerLayerWrapper(\n","            (layer): MultiHeadAttention(\n","              (linear_queries): Dense(512)\n","              (linear_keys): Dense(512)\n","              (linear_values): Dense(512)\n","              (linear_output): Dense(512)\n","            )\n","            (input_layer_norm): LayerNorm()\n","          )\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","      (1): SelfAttentionDecoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (attention): ListWrapper(\n","          (0): TransformerLayerWrapper(\n","            (layer): MultiHeadAttention(\n","              (linear_queries): Dense(512)\n","              (linear_keys): Dense(512)\n","              (linear_values): Dense(512)\n","              (linear_output): Dense(512)\n","            )\n","            (input_layer_norm): LayerNorm()\n","          )\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","      (2): SelfAttentionDecoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (attention): ListWrapper(\n","          (0): TransformerLayerWrapper(\n","            (layer): MultiHeadAttention(\n","              (linear_queries): Dense(512)\n","              (linear_keys): Dense(512)\n","              (linear_values): Dense(512)\n","              (linear_output): Dense(512)\n","            )\n","            (input_layer_norm): LayerNorm()\n","          )\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","      (3): SelfAttentionDecoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (attention): ListWrapper(\n","          (0): TransformerLayerWrapper(\n","            (layer): MultiHeadAttention(\n","              (linear_queries): Dense(512)\n","              (linear_keys): Dense(512)\n","              (linear_values): Dense(512)\n","              (linear_output): Dense(512)\n","            )\n","            (input_layer_norm): LayerNorm()\n","          )\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","      (4): SelfAttentionDecoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (attention): ListWrapper(\n","          (0): TransformerLayerWrapper(\n","            (layer): MultiHeadAttention(\n","              (linear_queries): Dense(512)\n","              (linear_keys): Dense(512)\n","              (linear_values): Dense(512)\n","              (linear_output): Dense(512)\n","            )\n","            (input_layer_norm): LayerNorm()\n","          )\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","      (5): SelfAttentionDecoderLayer(\n","        (self_attention): TransformerLayerWrapper(\n","          (layer): MultiHeadAttention(\n","            (linear_queries): Dense(512)\n","            (linear_keys): Dense(512)\n","            (linear_values): Dense(512)\n","            (linear_output): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","        (attention): ListWrapper(\n","          (0): TransformerLayerWrapper(\n","            (layer): MultiHeadAttention(\n","              (linear_queries): Dense(512)\n","              (linear_keys): Dense(512)\n","              (linear_values): Dense(512)\n","              (linear_output): Dense(512)\n","            )\n","            (input_layer_norm): LayerNorm()\n","          )\n","        )\n","        (ffn): TransformerLayerWrapper(\n","          (layer): FeedForwardNetwork(\n","            (inner): Dense(2048)\n","            (outer): Dense(512)\n","          )\n","          (input_layer_norm): LayerNorm()\n","        )\n","      )\n","    )\n","  )\n",")\n","\n","2023-07-22 00:29:53.121697: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2023-07-22 00:29:53.145000: I main.py:325] Using parameters:\n","data:\n","  eval_features_file: toy-ende/src-val.txt\n","  eval_labels_file: toy-ende/tgt-val.txt\n","  source_vocabulary: src-vocab.txt\n","  target_vocabulary: tgt-vocab.txt\n","  train_features_file: toy-ende/src-train.txt\n","  train_labels_file: toy-ende/tgt-train.txt\n","eval:\n","  batch_size: 32\n","  batch_type: examples\n","  length_bucket_width: 5\n","infer:\n","  batch_size: 32\n","  batch_type: examples\n","  length_bucket_width: 5\n","model_dir: run/\n","params:\n","  average_loss_in_time: true\n","  beam_width: 4\n","  decay_params:\n","    model_dim: 512\n","    warmup_steps: 8000\n","  decay_type: NoamDecay\n","  label_smoothing: 0.1\n","  learning_rate: 2.0\n","  num_hypotheses: 1\n","  optimizer: LazyAdam\n","  optimizer_params:\n","    beta_1: 0.9\n","    beta_2: 0.998\n","score:\n","  batch_size: 64\n","  batch_type: examples\n","  length_bucket_width: 5\n","train:\n","  average_last_checkpoints: 8\n","  batch_size: 3072\n","  batch_type: tokens\n","  effective_batch_size: 25000\n","  keep_checkpoint_max: 8\n","  length_bucket_width: 2\n","  max_step: 500000\n","  maximum_features_length: 100\n","  maximum_labels_length: 100\n","  sample_buffer_size: -1\n","  save_summary_steps: 100\n","\n","2023-07-22 00:29:53.712000: I inputter.py:316] Initialized source input layer:\n","2023-07-22 00:29:53.712000: I inputter.py:316]  - vocabulary size: 1001\n","2023-07-22 00:29:53.712000: I inputter.py:316]  - special tokens: BOS=no, EOS=no\n","2023-07-22 00:29:53.735000: I inputter.py:316] Initialized target input layer:\n","2023-07-22 00:29:53.735000: I inputter.py:316]  - vocabulary size: 1001\n","2023-07-22 00:29:53.735000: I inputter.py:316]  - special tokens: BOS=yes, EOS=yes\n","2023-07-22 00:29:53.763000: W runner.py:268] No checkpoint to restore in run/\n","2023-07-22 00:29:53.769000: W deprecation.py:350] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use eager execution and: \n","`tf.data.TFRecordDataset(path)`\n","2023-07-22 00:29:54.046000: W deprecation.py:350] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n","2023-07-22 00:29:58.970000: I main.py:325] Accumulate gradients of 9 iterations to reach effective batch size of 25000\n","2023-07-22 00:29:59.005000: I dataset_ops.py:2542] Training on 10000 examples\n","2023-07-22 00:30:40.413000: I runner.py:309] Number of model parameters: 45679081\n","2023-07-22 00:30:40.419000: I runner.py:309] Number of model weights: 260 (trainable = 260, non trainable = 0)\n","2023-07-22 00:30:43.209000: I training.py:176] Saved checkpoint run/ckpt-1\n"]}],"source":["!onmt-main --model_type Transformer --config data.yml --auto_config train --with_eval --num_gpus 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PK-1mIPbbX54"},"outputs":[],"source":["!onmt-main --config data.yml --auto_config infer --features_file toy-ende/src-test.txt > output.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dO0m3or4qYJQ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPShsf9Pm491FzgkQ7JKeBp"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}